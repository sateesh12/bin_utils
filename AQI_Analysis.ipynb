{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Author : Sateesh K\n",
    "#Date   : 17/May/2020\n",
    "#Purpose: As a part of UpX project, this is a submission to complete Capstone project\n",
    "#Guidelines: \n",
    "#Prepare the report in form of a power point presentation. Below\n",
    "#are the guidelines for the report.\n",
    "#a. Domain & topic of project\n",
    "#b. Introduction (brief info on project)\n",
    "#c. Dataset description\n",
    "#d. Business questions identified (at least 7-8 questions)\n",
    "#General format:\n",
    "#Question 1\n",
    "#Approach\n",
    "#Findings & Visualizations\n",
    "#e. Cleaning data and data imputation\n",
    "#f. Train-test data split\n",
    "#g. Model building, training and testing\n",
    "#h. Performance metrics\n",
    "#2. Use at least three algorithms learned from the Machine Learning track for the\n",
    "#project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data from .csv into a data-frame\n",
    "import pandas as pd\n",
    "initial_df = pd.read_csv('AirQualityUCI.csv')\n",
    "initial_df.head()\n",
    "initial_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for null values\n",
    "print(initial_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the null values of Date column to observe how the data looks.\n",
    "initial_df[initial_df['Date'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert meaningful values for Date NaN\n",
    "#Get 100 rows above the first NaN date value\n",
    "initial_df.loc[9337:9357,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Observing the data, each data has a data point per hour\n",
    "#However all the column of Date NaN also have a NaN\n",
    "#Hence it makes sense to drop all NaN rows which amount to 114 data points out of 9471 data points = 0.11%\n",
    "initial_df.dropna(subset=['Date'],inplace=True)\n",
    "initial_df.info()\n",
    "initial_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now check if the Date filed no longer as NaN\n",
    "initial_df[initial_df['Date'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All NaN in Date filed are dropped off, hence lost 114 rows out of 9471 rows of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now check over-all NaN in data-set\n",
    "#Check for null values\n",
    "print(initial_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Column 15 and 16 has no valid data, so need to dop them as well.\n",
    "initial_df.drop(initial_df.filter(regex=\"Unnamed\"),axis=1, inplace=True)\n",
    "initial_df.describe()\n",
    "initial_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As NaN in Date co-incided with NaN in other column, now there are no more NaN, Data is cleaned-up for further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy into a new Pandas Data-frame for clarity purposes only\n",
    "cleaned_up_df = initial_df\n",
    "initial_df.shape\n",
    "cleaned_up_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There is a problem with pandas_profile , hence ditching it. In fact pandas profile does not work even with a simple data-frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_up_df.head()\n",
    "cleaned_up_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove date and time for plotting purposes\n",
    "temp_df = cleaned_up_df.copy()\n",
    "temp_df.drop(columns=['Date','Time']) #This line causes error if executed mutiple times as \n",
    "#'Date' and 'Time' no longer esist after\n",
    "#first run\n",
    "temp_df.plot()\n",
    "cleaned_up_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From above graph it is clear that -200 is an invalid value which needs to be got rid of.\n",
    "#The idea is to drop if -200 is found in more than two columns\n",
    "#An attempt was made to drop all -200 values, but then the number of data points drop to from 9357 to 827, hence the logic\n",
    "#of dropping rows where two columns have -200.\n",
    "#filterinfDataframe = dfObj[(dfObj['Sale'] > 30) & (dfObj['Sale'] < 33) ]\n",
    "sub_set_df = cleaned_up_df[(cleaned_up_df['CO(GT)'] != -200) & (cleaned_up_df['PT08.S1(CO)'] != -200)]\n",
    "sub_set_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now you will see a more clean-plot\n",
    "sub_set_df.plot()\n",
    "sub_set_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the remaining -200s in the data-set to decide how to proceed\n",
    "print(\"In column CO(GT),  % of invalid values\", (sub_set_df['CO(GT)'] == -200).sum(axis=0)/len(sub_set_df)*100)\n",
    "print(\"In column PT08.S1(CO),  % of invalid values\", (sub_set_df['PT08.S1(CO)'] == -200).sum(axis=0)/len(sub_set_df)*100)\n",
    "print(\"In column NMHC(GT),  % of invalid values\", (sub_set_df['NMHC(GT)'] == -200).sum(axis=0)/len(sub_set_df)*100)\n",
    "print(\"In column C6H6(GT) % of invalid values\", (sub_set_df['C6H6(GT)'] == -200).sum(axis=0)/len(sub_set_df)*100)\n",
    "print(\"In column PT08.S2(NMHC) % of invalid values\", (sub_set_df['PT08.S2(NMHC)'] == -200).sum(axis=0)/len(sub_set_df)*100)\n",
    "print(\"In column NOx(GT) % of invalid values\", (sub_set_df['NOx(GT)'] == -200).sum(axis=0)/len(sub_set_df)*100)\n",
    "print(\"In column PT08.S3(NOx) % of invalid values\", (sub_set_df['PT08.S3(NOx)'] == -200).sum(axis=0)/len(sub_set_df)*100)\n",
    "print(\"In column NO2(GT) % of invalid values\", (sub_set_df['NO2(GT)'] == -200).sum(axis=0)/len(sub_set_df)*100)\n",
    "print(\"In column PT08.S4(NO2) % of invalid values\", (sub_set_df['PT08.S4(NO2)'] == -200).sum(axis=0)/len(sub_set_df)*100)\n",
    "print(\"In column PT08.S5(O3) % of invalid values\", (sub_set_df['PT08.S5(O3)'] == -200).sum(axis=0)/len(sub_set_df)*100)\n",
    "print(\"In column T % of invalid values\", (sub_set_df['T'] == -200).sum(axis=0)/len(sub_set_df)*100)\n",
    "print(\"In column RH % of invalid values\", (sub_set_df['RH'] == -200).sum(axis=0)/len(sub_set_df)*100)\n",
    "print(\"In column AH % of invalid values\", (sub_set_df['AH'] == -200).sum(axis=0)/len(sub_set_df)*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As the NMHC(GT) contains 88% -200 values, this column is dropped off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the column NMHC(GT)\n",
    "sub_set_with_minimal_minus_200 = sub_set_df.drop(['NMHC(GT)'],axis=1)\n",
    "sub_set_with_minimal_minus_200.shape\n",
    "sub_set_with_minimal_minus_200.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Common -200 values in NOx(GT) and NO2(GT) can be removed as well.\n",
    "fully_clean_df = sub_set_with_minimal_minus_200[(sub_set_with_minimal_minus_200['NOx(GT)'] != -200) & (sub_set_with_minimal_minus_200['NO2(GT)'] != -200)]\n",
    "fully_clean_df.plot()\n",
    "fully_clean_df .shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hypothesis 1\n",
    "#Relative humidity and temperature are positively co-related\n",
    "import seaborn as sns\n",
    "sns.relplot(x='T', y='RH', data=fully_clean_df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Air temperature and Relative humidity are negatively co-related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hypothesis 2: Absolute Humidity is negatively co-related to temperature\n",
    "import seaborn as sns\n",
    "sns.relplot(x='T', y='AH', data=fully_clean_df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As temperature increases Absolute humidity also increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hypothesis 3: Absolute and relative humidity are positively co-related\n",
    "import seaborn as sns\n",
    "sns.relplot(x='RH', y='AH', hue='T', data=fully_clean_df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Absolute and Relative humidity are positively co-related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hypothesis 4: CO and NO2 are produced due to combustion in vehciles. They both are formed from very similar reactions, \n",
    "#hence they must be positively co-related.\n",
    "import seaborn as sns\n",
    "sns.relplot(x='PT08.S1(CO)', y='PT08.S4(NO2)', data=fully_clean_df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hypothesis 5: NO compounds cause creation of secondary pollutants like O3, hence NO and O3 should be positively co-related.\n",
    "import seaborn as sns\n",
    "sns.relplot(x='PT08.S4(NO2)', y='PT08.S5(O3)', data=fully_clean_df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hypothesis 6: NOx and NO2 are similar family compounds hence will be positively co-related\n",
    "import seaborn as sns\n",
    "sns.relplot(x='PT08.S3(NOx)', y='PT08.S4(NO2)', data=fully_clean_df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hypothesis 7: The composition of vehicular pollutants in terms of % is as follows:\n",
    "#HC 1.75g/km\n",
    "#CO 20.9 g/km\n",
    "#NOx 1.39g/km\n",
    "#CO2 258g/km\n",
    "\n",
    "#Based on above composition, and the fact that the sensors were placed on a busy street junction\n",
    "#Ratio of CO to NOx should always be greater than 1.\n",
    "import seaborn as sns\n",
    "temp_df['Pollution_Ratio'] = fully_clean_df['PT08.S1(CO)'] / fully_clean_df['PT08.S3(NOx)']\n",
    "sns.distplot(temp_df['Pollution_Ratio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if the sensors are data points are positively co-related to GT values.\n",
    "#If this is the  case then the Ground Truth can be removed from the ML models as they exist only to verify sensor data.\n",
    "import seaborn as sns\n",
    "sns.relplot(x='CO(GT)', y='PT08.S1(CO)', data=fully_clean_df);\n",
    "sns.relplot(x='NOx(GT)', y='PT08.S3(NOx)', data=fully_clean_df);\n",
    "sns.relplot(x='NO2(GT)', y='PT08.S4(NO2)', data=fully_clean_df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop all the GT values for ML model building. \n",
    "#Note C6H6 is kept as is, as this there is no sensor value for the same.\n",
    "print(fully_clean_df.shape)\n",
    "ml_data_set = fully_clean_df.drop(['CO(GT)','NOx(GT)','NO2(GT)'],axis=1)\n",
    "print(ml_data_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the date and time so that ML models can use them to learn.\n",
    "#For e.g. months are needed to take into account seasonality in Italy\n",
    "#Spring Mar-May\n",
    "#Summer June-Aug\n",
    "#Autumn Sep-Nov\n",
    "#Winter Dec-Feb\n",
    "#The format of date is M/DD/YYYY or M-DD-YY\n",
    "#First is to make all date formats uniform\n",
    "type(ml_data_set['Date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
