{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Author : Sateesh K\n",
    "#Date   : 17/May/2020\n",
    "#Purpose: As a part of UpX project, this is a submission to complete Capstone project\n",
    "#Guidelines: \n",
    "#Prepare the report in form of a power point presentation. Below\n",
    "#are the guidelines for the report.\n",
    "#a. Domain & topic of project\n",
    "#b. Introduction (brief info on project)\n",
    "#c. Dataset description\n",
    "#d. Business questions identified (at least 7-8 questions)\n",
    "#General format:\n",
    "#Question 1\n",
    "#Approach\n",
    "#Findings & Visualizations\n",
    "#e. Cleaning data and data imputation\n",
    "#f. Train-test data split\n",
    "#g. Model building, training and testing\n",
    "#h. Performance metrics\n",
    "#2. Use at least three algorithms learned from the Machine Learning track for the\n",
    "#project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the data from .csv into a data-frame\n",
    "import pandas as pd\n",
    "initial_df = pd.read_csv('AirQualityUCI.csv')\n",
    "initial_df.head()\n",
    "initial_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for null values\n",
    "print(initial_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the null values of Date column to observe how the data looks.\n",
    "initial_df[initial_df['Date'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert meaningful values for Date NaN\n",
    "#Get 100 rows above the first NaN date value\n",
    "initial_df.loc[9337:9357,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Observing the data, each data has a data point per hour\n",
    "#However all the column of Date NaN also have a NaN\n",
    "#Hence it makes sense to drop all NaN rows which amount to 114 data points out of 9471 data points = 0.11%\n",
    "initial_df.dropna(subset=['Date'],inplace=True)\n",
    "initial_df.info()\n",
    "initial_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now check if the Date filed no longer as NaN\n",
    "initial_df[initial_df['Date'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All NaN in Date filed are dropped off, hence lost 114 rows out of 9471 rows of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now check over-all NaN in data-set\n",
    "#Check for null values\n",
    "print(initial_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Column 15 and 16 has no valid data, so need to dop them as well.\n",
    "initial_df.drop(initial_df.filter(regex=\"Unnamed\"),axis=1, inplace=True)\n",
    "initial_df.describe()\n",
    "initial_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As NaN in Date co-incided with NaN in other column, now there are no more NaN, Data is cleaned-up for further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy into a new Pandas Data-frame for clarity purposes only\n",
    "cleaned_up_df = initial_df\n",
    "initial_df.shape\n",
    "cleaned_up_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#There is a problem with pandas_profile , hence ditching it. In fact pandas profile does not work even with a simple data-frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_up_df.head()\n",
    "cleaned_up_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove date and time for plotting purposes\n",
    "temp_df = cleaned_up_df.copy()\n",
    "temp_df.drop(columns=['Date','Time']) #This line causes error if executed mutiple times as \n",
    "#'Date' and 'Time' no longer esist after\n",
    "#first run\n",
    "temp_df.plot()\n",
    "cleaned_up_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From above graph it is clear that -200 is an invalid value which needs to be got rid of.\n",
    "#The idea is to drop if -200 is found in more than two columns\n",
    "#An attempt was made to drop all -200 values, but then the number of data points drop to from 9357 to 827, hence the logic\n",
    "#of dropping rows where two columns have -200.\n",
    "#filterinfDataframe = dfObj[(dfObj['Sale'] > 30) & (dfObj['Sale'] < 33) ]\n",
    "sub_set_df = cleaned_up_df[(cleaned_up_df['CO(GT)'] != -200) & (cleaned_up_df['PT08.S1(CO)'] != -200)]\n",
    "sub_set_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now you will see a more clean-plot\n",
    "sub_set_df.plot()\n",
    "sub_set_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the remaining -200s in the data-set to decide how to proceed\n",
    "print(\"In column CO(GT),  % of invalid values\", (sub_set_df['CO(GT)'] == -200).sum(axis=0)/len(sub_set_df)*100)\n",
    "print(\"In column PT08.S1(CO),  % of invalid values\", (sub_set_df['PT08.S1(CO)'] == -200).sum(axis=0)/len(sub_set_df)*100)\n",
    "print(\"In column NMHC(GT),  % of invalid values\", (sub_set_df['NMHC(GT)'] == -200).sum(axis=0)/len(sub_set_df)*100)\n",
    "print(\"In column C6H6(GT) % of invalid values\", (sub_set_df['C6H6(GT)'] == -200).sum(axis=0)/len(sub_set_df)*100)\n",
    "print(\"In column PT08.S2(NMHC) % of invalid values\", (sub_set_df['PT08.S2(NMHC)'] == -200).sum(axis=0)/len(sub_set_df)*100)\n",
    "print(\"In column NOx(GT) % of invalid values\", (sub_set_df['NOx(GT)'] == -200).sum(axis=0)/len(sub_set_df)*100)\n",
    "print(\"In column PT08.S3(NOx) % of invalid values\", (sub_set_df['PT08.S3(NOx)'] == -200).sum(axis=0)/len(sub_set_df)*100)\n",
    "print(\"In column NO2(GT) % of invalid values\", (sub_set_df['NO2(GT)'] == -200).sum(axis=0)/len(sub_set_df)*100)\n",
    "print(\"In column PT08.S4(NO2) % of invalid values\", (sub_set_df['PT08.S4(NO2)'] == -200).sum(axis=0)/len(sub_set_df)*100)\n",
    "print(\"In column PT08.S5(O3) % of invalid values\", (sub_set_df['PT08.S5(O3)'] == -200).sum(axis=0)/len(sub_set_df)*100)\n",
    "print(\"In column T % of invalid values\", (sub_set_df['T'] == -200).sum(axis=0)/len(sub_set_df)*100)\n",
    "print(\"In column RH % of invalid values\", (sub_set_df['RH'] == -200).sum(axis=0)/len(sub_set_df)*100)\n",
    "print(\"In column AH % of invalid values\", (sub_set_df['AH'] == -200).sum(axis=0)/len(sub_set_df)*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As the NMHC(GT) contains 88% -200 values, this column is dropped off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the column NMHC(GT)\n",
    "sub_set_with_minimal_minus_200 = sub_set_df.drop(['NMHC(GT)'],axis=1)\n",
    "sub_set_with_minimal_minus_200.shape\n",
    "sub_set_with_minimal_minus_200.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Common -200 values in NOx(GT) and NO2(GT) can be removed as well.\n",
    "fully_clean_df = sub_set_with_minimal_minus_200[(sub_set_with_minimal_minus_200['NOx(GT)'] != -200) & (sub_set_with_minimal_minus_200['NO2(GT)'] != -200)]\n",
    "fully_clean_df.plot()\n",
    "fully_clean_df .shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hypothesis 1\n",
    "#Relative humidity and temperature are positively co-related\n",
    "import seaborn as sns\n",
    "sns.relplot(x='T', y='RH', data=fully_clean_df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Air temperature and Relative humidity are negatively co-related"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hypothesis 2: Absolute Humidity is negatively co-related to temperature\n",
    "import seaborn as sns\n",
    "sns.relplot(x='T', y='AH', data=fully_clean_df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As temperature increases Absolute humidity also increases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hypothesis 3: Absolute and relative humidity are positively co-related\n",
    "import seaborn as sns\n",
    "sns.relplot(x='RH', y='AH', hue='T', data=fully_clean_df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Absolute and Relative humidity are positively co-related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hypothesis 4: CO and NO2 are produced due to combustion in vehciles. They both are formed from very similar reactions, \n",
    "#hence they must be positively co-related.\n",
    "import seaborn as sns\n",
    "sns.relplot(x='PT08.S1(CO)', y='PT08.S4(NO2)', data=fully_clean_df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hypothesis 5: NO compounds cause creation of secondary pollutants like O3, hence NO and O3 should be positively co-related.\n",
    "import seaborn as sns\n",
    "sns.relplot(x='PT08.S4(NO2)', y='PT08.S5(O3)', data=fully_clean_df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hypothesis 6: NOx and NO2 are similar family compounds hence will be positively co-related\n",
    "import seaborn as sns\n",
    "sns.relplot(x='PT08.S3(NOx)', y='PT08.S4(NO2)', data=fully_clean_df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hypothesis 7: The composition of vehicular pollutants in terms of % is as follows:\n",
    "#HC 1.75g/km\n",
    "#CO 20.9 g/km\n",
    "#NOx 1.39g/km\n",
    "#CO2 258g/km\n",
    "\n",
    "#Based on above composition, and the fact that the sensors were placed on a busy street junction\n",
    "#Ratio of CO to NOx should always be greater than 1.\n",
    "import seaborn as sns\n",
    "temp_df['Pollution_Ratio'] = fully_clean_df['PT08.S1(CO)'] / fully_clean_df['PT08.S3(NOx)']\n",
    "sns.distplot(temp_df['Pollution_Ratio'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if the sensors are data points are positively co-related to GT values.\n",
    "#If this is the  case then the Ground Truth can be removed from the ML models as they exist only to verify sensor data.\n",
    "import seaborn as sns\n",
    "sns.relplot(x='CO(GT)', y='PT08.S1(CO)', data=fully_clean_df);\n",
    "sns.relplot(x='NOx(GT)', y='PT08.S3(NOx)', data=fully_clean_df);\n",
    "sns.relplot(x='NO2(GT)', y='PT08.S4(NO2)', data=fully_clean_df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop all the GT values for ML model building. \n",
    "#Note C6H6 is kept as is, as this there is no sensor value for the same.\n",
    "print(fully_clean_df.shape)\n",
    "ml_data_set = fully_clean_df.drop(['CO(GT)','NOx(GT)','NO2(GT)'],axis=1)\n",
    "print(ml_data_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the date and time so that ML models can use them to learn.\n",
    "#For e.g. months are needed to take into account seasonality in Italy\n",
    "#Spring Mar-May\n",
    "#Summer June-Aug\n",
    "#Autumn Sep-Nov\n",
    "#Winter Dec-Feb\n",
    "#The format of date is M/DD/YYYY or M-DD-YY\n",
    "#First is to make all date formats uniform\n",
    "import pandas as pd\n",
    "ml_data_set['Month'] = pd.to_datetime(ml_data_set['Date']).dt.month\n",
    "ml_data_set['Year'] =  pd.to_datetime(ml_data_set['Date']).dt.year\n",
    "#Drop the Date column as month which is needed is extracted.\n",
    "ml_data_set.drop(['Date'],axis=1,inplace = True)\n",
    "\n",
    "#Now extract the hour from Time column and drop the same.\n",
    "ml_data_set['Hour'] = pd.to_datetime(ml_data_set['Time']).dt.hour\n",
    "ml_data_set.drop(['Time'],axis=1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First ML model for this problem, going with Linear Regression to predict RH\n",
    "ml_data_set.describe()\n",
    "X = ml_data_set.drop(['RH'],axis = 1)\n",
    "y = ml_data_set['RH']\n",
    "X.head()\n",
    "#y.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_data_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=1234,test_size=0.3)\n",
    "print(X_train.head())\n",
    "y_train\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Model 1 : Simple Linear Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train, y_train)\n",
    "\n",
    "print(linreg.coef_)                                            # Coefficients for Logistic Regression\n",
    "print(linreg.intercept_)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict using the linear regression model\n",
    "y_pred = linreg.predict(X_test)\n",
    "y_pred.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate these metrics by hand!\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "def typical_linear_model_performance(y_pred):\n",
    "    print ('MAE:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "    print ('MSE:', metrics.mean_squared_error(y_test, y_pred))\n",
    "    print ('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred))) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the R2 value\n",
    "from sklearn.model_selection import cross_val_score\n",
    "def get_cross_value_score(model):\n",
    "    scores = cross_val_score(model, X_train, y_train,cv=5,scoring='r2')\n",
    "    print('CV Mean: ', np.mean(scores))\n",
    "    print('STD: ', np.std(scores))\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cross_value_score(linreg)\n",
    "typical_linear_model_performance(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Ridge Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "from sklearn.linear_model import Ridge\n",
    "# Train model with default alpha=1\n",
    "ridge = Ridge(alpha=1).fit(X_train, y_train)\n",
    "\n",
    "# get cross val scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let the model do its magic\n",
    "y_pred = ridge.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Measure the model performance\n",
    "get_cross_value_score(ridge)\n",
    "typical_linear_model_performance(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use grid search to find a suitable alpha value\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "alpha = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "param_grid = dict(alpha=alpha)\n",
    "grid = GridSearchCV(estimator=ridge, param_grid=param_grid, scoring='r2', verbose=1, n_jobs=-1)\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "print('Best Score: ', grid_result.best_score_)\n",
    "print('Best Params: ', grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train --- > #Model Magic ---- > #Measure Performance of model\n",
    "from sklearn.linear_model import Ridge\n",
    "# Train model with default alpha=1\n",
    "ridge = Ridge(alpha=0.1).fit(X_train, y_train)\n",
    "y_pred = ridge.predict(X_test)\n",
    "# get cross val scores\n",
    "get_cross_value_score(ridge)\n",
    "typical_linear_model_performance(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3: Lasso Linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train --> Model Magic ---> Check Performance\n",
    "from sklearn.linear_model import Lasso\n",
    "# Train model with default alpha=1\n",
    "lasso = Lasso(alpha=1).fit(X_train, y_train)\n",
    "\n",
    "y_pred = lasso.predict(X_test)\n",
    "\n",
    "# get cross val scores\n",
    "get_cross_value_score(lasso)\n",
    "typical_linear_model_performance(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After applying Laso regression check which co-eff have become zero in the model\n",
    "for coef, col in enumerate(X_train.columns):\n",
    "    print(f'{col}:  {lasso.coef_[coef]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
